\chapter{Novel Approaches and Proof of Validity}\label{chap:algorithms}

This chapter presents our proposed approaches for solving general \(k\)-cluster problems in one dimension. We introduce two algorithms: the \textit{\(k\)-cluster algorithm} and the \textit{2-cluster algorithm}. Both methods exploit the structure of one-dimensional data and utilize sorting, prefix sums, and binary search to achieve significant computational efficiency compared to traditional clustering methods.

\paragraph{Finding Cluster Boundaries:}
For one-dimensional data, determining a point's cluster assignment involves identifying the interval it falls into. The boundaries between clusters are the arithmetic midpoints of consecutive centroids. When both the data and centroids are sorted, these boundaries can be efficiently located using binary search, requiring $O(k \cdot \log n)$ time. If centroids need sorting, an additional $O(k \cdot \log k)$ time is needed; however, as $k \leq n$, the total time remains $O(k \cdot \log n)$. This approach forms the basis for subsequent optimizations.

\section{The \(k\)-Cluster Algorithm}\label{sec:k_cluster_algorithm}

The \(k\)-cluster algorithm is a one-dimensional adaptation of greedy $k$-means++ initialization followed by Lloyd’s algorithm iterations, and can be defined for both weighted and unweighted data. The algorithm leverages:

\begin{enumerate}
    \item \textbf{Sorted Data:} Sorting the input array \(X\) of size \(n\) in ascending order allows quick determination of cluster assignments via binary search on cluster boundaries.
    \item \textbf{Prefix Sums:} Precomputing prefix sums enables constant-time computation of weighted and unweighted sums, means, and inertia values over arbitrary intervals. Specifically, for weighted data:
    \[
    W[j] = \sum_{i=1}^j w_i, \quad 
    (WX)[j] = \sum_{i=1}^j w_i x_i, \quad 
    (WX^2)[j] = \sum_{i=1}^j w_i x_i^2.
    \]
    For unweighted data, this simplifies to:
    \[
    X^{(1)}[j] = \sum_{i=1}^j x_i, \quad X^{(2)}[j] = \sum_{i=1}^j x_i^2.
    \]
    \item \textbf{Binary Search:} Binary search is central to efficiently determining cluster boundaries and performing weighted random sampling during initialization.
\end{enumerate}

\subsection{WCSS and Prefix Sums}

The within-cluster sum of squares (WCSS) for weighted data is defined as:
\[
\text{WCSS} = \sum_{i=1}^k \sum_{x_j \in C_i} w_j (x_j - \mu_i)^2,
\]
where \(C_i\) is the \(i\)-th cluster, \(\mu_i\) is its centroid, and \(w_j\) is the weight of point \(x_j\). Expanding the squared term:
\[
(x_j - \mu_i)^2 = x_j^2 - 2x_j\mu_i + \mu_i^2,
\]
yields:
\[
\text{WCSS}_i = \sum_{x_j \in C_i} w_j x_j^2 - 2\mu_i \sum_{x_j \in C_i} w_j x_j + \mu_i^2 \sum_{x_j \in C_i} w_j.
\]

Using prefix sums:
\[
\begin{aligned}
\sum_{x_j \in C_i} w_j x_j^2 &= (WX^2)[b_i] - (WX^2)[b_{i-1}], \\
\sum_{x_j \in C_i} w_j x_j &= (WX)[b_i] - (WX)[b_{i-1}], \\
\sum_{x_j \in C_i} w_j &= W[b_i] - W[b_{i-1}].
\end{aligned}
\]

where \(b_{i-1}\) and \(b_i\) are the cluster boundaries. The centroid is:
\[
\mu_i = \frac{(WX)[b_i] - (WX)[b_{i-1}]}{W[b_i] - W[b_{i-1}]}.
\]

All these queries take \(O(1)\) time per cluster once the prefix sums are computed. Hence, WCSS and centroid calculations are efficient, requiring only \(O(k)\) time across all $k$ clusters, if cluster boundaries are known. Determining cluster boundaries costs \(O(k \log n)\), so the total cost for WCSS calculation given centroids is \(O(k \log n)\).

\subsection{Greedy $k$-Means++ Initialization}

The $k$-means++ initialization selects centroids such that new centroids are chosen with probabilities proportional to their squared distances from the closest existing centroid. Our method efficiently implements this using \textbf{binary search} combined with \textbf{cumulative sum queries}.

\paragraph{Steps for Initialization:}
\begin{enumerate}
    \item \textbf{First Centroid Selection:}
    The first centroid is chosen randomly, weighted by the point weights \(w_j\). To achieve this:
    \begin{itemize}
        \item Given the cumulative sum of weights \(W[j] = \sum_{i=1}^j w_i\),
        \item Generate a random number \(r \in [0, W[n]]\),
        \item Perform binary search on \(W\) to find the smallest $j$ such that $W[j] \geq r$, and thus the corresponding point \(x_j\). This step costs \(O(\log n)\).
    \end{itemize}

    \item \textbf{Subsequent Centroid Selection:}
    For each new centroid:
    \begin{enumerate}
        \item \textbf{Binary Search for Cluster Assignments:}
        Given the existing centroids, determine the cluster boundaries using a binary search of consecutive centroid midpoints. This step costs \(O(k \log n)\).

        \item \textbf{Cumulative Sum for Squared Distances:}
        To sample a new centroid, we need the cumulative sum of squared distances \(D_i^2\), where \(D_i\) is the distance of \(x_i\) to its closest centroid. The cumulative sum \(S[j]\) is defined as:
        \[
        S[j] = \sum_{i=1}^j D_i^2.
        \]
        \textbf{Importantly:} \(S\) is not explicitly constructed. Instead for each query $j$ on $S$:
        \begin{itemize}
            \item The sum of squared distances \(D_i^2\) are obtained using prefix sums over the \(k\) clusters, up to the $j$th point. This is equivalent to calculating the WCSS up to the $j$th point. For each cluster, this sum can be retrieved in $O(1)$ time, as the cluster boundaries are known. 
            \item Querying \(S[j]\) for any \(j\) requires \(O(k)\) time, as it aggregates contributions from all relevant clusters.
        \end{itemize}

        \item \textbf{Binary Search on \(S\):}
        \begin{itemize}
            \item Generate a random number \(r \in [0, S[n]]\),
            \item Perform binary search on \(S\) to find the smallest \(j\) such that \(S[j] \geq r\).
        \end{itemize}
        Each binary search involves \(O(\log n)\) queries of \(S\), where each query takes \(O(k)\). Thus, the total cost for sampling one new centroid is \(O(k \cdot \log n)\), and for $l$ candidates, $O(l \cdot k \cdot \log n)$.

        \item \textbf{Greedy Candidate Selection:}
        For each candidate:
        \begin{itemize}
            \item Update cluster boundaries using binary search (\(O(k \cdot \log n)\)),
            \item Compute the total WCSS using prefix sums (\(O(k)\)).
        \end{itemize}
        The candidate minimizing the total WCSS is selected as the next centroid. For \(l\) candidates, this step costs \(O(l \cdot k \cdot \log n)\).

        \item \textbf{Combined Initialization Time Complexity:}
        Combining the steps, the total cost for generating and evaluating \(l\) candidates per new centroid is $O(k \cdot \log n \cdot + l \cdot k \cdot \log n + l \cdot k \cdot \log n) = O(l \cdot k \cdot \log n)$.
    \end{enumerate}
\end{enumerate}

\subsection{Complexity Analysis}

The overall time complexity of the \(k\)-cluster algorithm is as follows:

\paragraph{Greedy $k$-Means++ Initialization:}
As detailed in the previous section:
\begin{itemize}
    \item Selecting the first centroid using weighted sampling costs \(O(\log n)\),
    \item Each subsequent centroid requires \(O(l \cdot k \cdot \log n)\), where \(l\) is the number of local trials.
\end{itemize}
The total cost for initialization across \(k\) centroids is therefore:
\[
O(l \cdot k^2 \cdot \log n).
\]

\paragraph{Lloyd’s Algorithm Iterations:}
Each iteration of Lloyd’s algorithm consists of:
\begin{itemize}
    \item Updating cluster boundaries via binary search: \(O(k \cdot \log n)\),
    \item Updating centroids using prefix sums: \(O(k)\).
\end{itemize}
For \(i\) iterations, the total cost is:
\[
O(i \cdot k \cdot \log n).
\]

\paragraph{Overall Time Complexity:}
The combined cost of greedy $k$-means++ initialization and Lloyd’s algorithm is:
\[
O(l \cdot k^2  \cdot \log n) + O(i \cdot k \cdot \log n).
\]

This does not account for the initial overhead of sorting the data and calculating prefix sums, which cost $O(n \log n)$ and $O(n)$, respectively.

Comparing against conventional implementations of $O(l \cdot k \cdot n) + O(i \cdot k \cdot n)$, note how the dependence on $n$ (dataset size) has decreased, at the cost of quadratic complexity in $k$ during initialization. However, since $k \ll n$ in most practical cases, this tradeoff is justified. For experimental speedup proofs, see Chapter~\ref{chap:experiments}.

\section{The 2-Cluster Algorithm}

For the 2-cluster problem in one-dimensional sorted data, the task reduces to finding a single \textbf{cluster boundary} that divides the data into two contiguous clusters. To efficiently locate this boundary, we iteratively refine a \textbf{search scope} to identify the correct \textbf{division interval}. A division interval is defined as the interval between two consecutive points in the sorted data that contains the cluster boundary.

\textbf{Note:} For all discussions in this section, all notions of direction (i.e., left or right) are with respect to the one-dimensional coordinate axis along which the data points are sorted. Thus, \emph{left} refers to decreasing \(x\)-values and \emph{right} refers to increasing \(x\)-values.

\subsection{Definitions and Key Observations}

\begin{itemize}
    \item A \textbf{division interval} is defined as the interval between two consecutive points \(x_{\text{div\_left}}\) and \(x_{\text{div\_right}}\) in sorted data that contains the cluster boundary (note that, of course, $\text{div\_left} + 1 = \text{div\_right}$).
    \item The \textbf{midpoint} for a division interval is defined as:
    \[
    \text{Midpoint} = \frac{\mu_{\text{left}} + \mu_{\text{right}}}{2},
    \]
    where \(\mu_{\text{left}}\) and \(\mu_{\text{right}}\) are the centroids of the left and right clusters defined by the division interval, respectively. These centroids are computed with prefix sums, using:
    \[
    \mu_{\text{left}} = \frac{\sum_{i=1}^{\text{div\_left}} w_i x_i}{\sum_{i=1}^{\text{div\_left}} w_i}, \quad 
    \mu_{\text{right}} = \frac{\sum_{i=\text{div\_right}}^n w_i x_i}{\sum_{i=\text{div\_right}}^n w_i}.
    \]
    The prefix sums $W$ and $WX$, as defined for the $k$-cluster algorithm, allow this calculation to be done in $O(1)$ time.
    \item A division interval is classified as follows:
        \begin{itemize}
            \item \textbf{Right-pointing:} The midpoint lies to the right of \(x_{\text{div\_right}}\).
            \item \textbf{Left-pointing:} The midpoint lies to the left of \(x_{\text{div\_left}}\).
            \item \textbf{Convergent:} The midpoint lies within the division interval itself, indicating a Lloyd’s algorithm convergence.
        \end{itemize}
    Note that every division interval can be classified into exactly one of these three categories.

    \item The \textbf{search scope} refers to the range of candidate division intervals, which is iteratively refined during the binary search to locate a convergent interval.
\end{itemize}

\subsection{Algorithm Description}

The algorithm aims to identify the correct division interval (i.e., a convergent interval) using binary search. The key steps are as follows:

\begin{enumerate}
    \item \textbf{Initialize the Search Scope:}  
    Start with the whole scope---that is, all possible division intervals ranging from the first interval \([x_1, x_2]\) to the last interval \([x_{n-1}, x_n]\).

    \item \textbf{Iteratively Query the Center Interval:}  
    At each step:
    \begin{itemize}
        \item Select the \textbf{center division interval} within the current search scope.
        \item Compute the centroids \(\mu_{\text{left}}\) and \(\mu_{\text{right}}\) and calculate the midpoint:
        \[
        \text{Midpoint} = \frac{\mu_{\text{left}} + \mu_{\text{right}}}{2}.
        \]
    \end{itemize}

    \item \textbf{Refine the Search Scope:}
    Compare the midpoint to the endpoints $x_{\text{div\_left}}$ and $x_{\text{div\_right}}$ of the queried division interval:
    \begin{itemize}
        \item If the interval is \textbf{right-pointing}, exclude all intervals to the left of the current interval, including itself.
        \item If the interval is \textbf{left-pointing}, exclude all intervals to the right of the current interval, including itself.
        \item If the interval is \textbf{convergent}, terminate; the cluster boundary has been found.
    \end{itemize}

    \item \textbf{Repeat Until Convergence:}  
    Continue the process until a convergent interval is found.
\end{enumerate}

The binary search guarantees that the number of candidate intervals is halved at each iteration, ensuring \(O(\log n)\) convergence.

\subsection{Proof of Validity}

To prove the correctness of the algorithm, we rely on the monotonic behavior of the centroids’ midpoint and the structure of the division intervals.

\paragraph{1. Monotonic Behavior of the Midpoint:}  
When the division interval \([x_{\text{div\_left}}, x_{\text{div\_right}}]\) is shifted one step to the right, i.e., to \([x_{\text{div\_right}}, x_{\text{div\_right} + 1}]\):
\begin{itemize}
    \item The point \(x_{\text{div\_right}}\), which was previously the leftmost point in the right cluster, is excluded from the right cluster and added to the left cluster.
    \item This change causes the \textbf{left centroid} \(\mu_{\text{left}}\) to increase, as a point with a larger value has been included in the left cluster.
    \item Simultaneously, the \textbf{right centroid} \(\mu_{\text{right}}\) also increases, as the smallest point in the right cluster has been removed.
    \item Since both centroids increase, the new \textbf{midpoint}:
    \[
    \text{Midpoint}^{\text{new}} = \frac{\mu_{\text{left}}^{\text{new}} + \mu_{\text{right}}^{\text{new}}}{2}
    \]
    is greater than the old midpoint:
    \[
    \text{Midpoint}^{\text{new}} > \text{Midpoint}^{\text{old}}.
    \]
\end{itemize}

Similarly, shifting the division interval one step to the left causes the midpoint to strictly decrease.

\paragraph{2. Behavior of Right-Pointing and Left-Pointing Intervals:}  
The monotonic behavior of the midpoint ensures the following:

\begin{itemize}
    \item \textbf{Right-Pointing Intervals:}  
    Suppose a division interval \([x_{\text{div\_left}}, x_{\text{div\_right}}]\) is right-pointing, so:
    \[
    x_{\text{div\_right}} < \text{Midpoint}^{\text{old}}.
    \]
    After shifting the interval one step to the right to the new interval \([x_{\text{div\_right}}, x_{\text{div\_right+1}}]\), the corresponding new midpoint strictly increases:
    \[
    x_{\text{div\_right}} < \text{Midpoint}^{\text{old}} < \text{Midpoint}^{\text{new}}.
    \]
    For the new interval  \([x_{\text{div\_right}}, x_{\text{div\_right+1}}]\) to become left-pointing, the new midpoint, $\text{Midpoint}^{\text{new}}$, would have to be less than \(x_{\text{div\_right}}\). But directly from the inequality above:
    \[
    x_{\text{div\_right}} < \text{Midpoint}^{\text{new}}.
    \]
    Thus, a right-pointing interval cannot suddenly become left-pointing after a rightward shift; it remains right-pointing or becomes convergent.

    \item \textbf{Left-Pointing Intervals:}  
    By a symmetric argument, for a left-pointing interval, shifting one step to the left strictly decreases the midpoint, ensuring it cannot suddenly become right-pointing. Instead, it remains left-pointing or becomes convergent.
\end{itemize}

This ensures that from the perspective of shifting the intervals (one step at a time in the chosen direction), right-pointing and left-pointing intervals cannot directly switch roles in a single move.

\paragraph{Convergent Interval in a Search Scope}
In any search scope where the first interval is right-pointing and the last interval is left-pointing, there must exist at least one convergent interval between them. This follows from the fact that a right-pointing interval cannot immediately precede a left-pointing interval when moving stepwise to the right.

\paragraph{Convergence of the Binary Search}
For the entire search scope, the first division interval (between \(x_1\) and \(x_2\)) is either right-pointing or convergent, and the last division interval (between \(x_{n-1}\) and \(x_n\)) is either left-pointing or convergent. Therefore, by the principle established above, there must exist at least one convergent interval within the entire scope. The previously detailed binary search iteratively reduces this scope in a way such that a convergent interval is always included, eventually narrowing the scope to a single convergent interval.

\subsection{Limitations}
While the monotonicity argument ensures that right-pointing and left-pointing intervals cannot directly switch roles when shifting in the direction they point (rightward for right-pointing, leftward for left-pointing), this guarantee does not hold when shifting in the opposite direction. Multiple local minima in Lloyd’s algorithm can produce patterns like:
\[
RRRCLLLRRRCLLL,
\]
where \(R\) denotes right-pointing, \(L\) denotes left-pointing, and \(C\) denotes convergent intervals. In such scenarios:
\begin{itemize}
    \item Moving a left-pointing interval to the right can produce a right-pointing interval (and vice versa).
    \item The algorithm still finds at least one convergent interval in \(O(\log n)\) time, but the found cluster boundary may correspond to a local minimum rather than the global optimum.
\end{itemize}

\subsection{Algorithm Guarantees}

By leveraging the monotonic behavior of the midpoint and the fact that a convergent interval must exist between the first right-pointing and last left-pointing intervals, the binary search efficiently identifies a convergent interval representing a cluster boundary. The algorithm achieves this in \(O(\log n)\) time without relying on iterative steps of Lloyd's algorithm.

In the presence of multiple local minima, the algorithm is guaranteed to converge to a valid solution, though it may not necessarily find the global optimum. Similar to the \(k\)-cluster algorithm, an initial preprocessing step is required, which includes sorting the data in \(O(n \log n)\) time and computing prefix sums in \(O(n)\) time.


\section{Summary}

In this chapter, we introduced two novel algorithms for solving the \(k\)-cluster problem in one-dimensional sorted data: the \textbf{\(k\)-cluster algorithm} and the \textbf{2-cluster algorithm}. Both methods leverage key observations about the structure of one-dimensional data to achieve significant computational efficiency.

\begin{itemize}
    \item The \textbf{\(k\)-cluster algorithm} combines greedy $k$-means++ initialization with efficient Lloyd’s iterations. By exploiting sorted data, prefix sums, and binary search:
        \begin{itemize}
            \item The initialization process achieves a complexity of \(O(l \cdot k^2 \cdot \log n)\), where \(k\) is the number of clusters and \(l\) is the number of local trials.
            \item Each iteration of Lloyd’s algorithm requires \(O(k \log n)\) time, ensuring efficient updates of cluster boundaries and centroids.
        \end{itemize}

    \item The \textbf{2-cluster algorithm} focuses on the specific case of \(k=2\), where the problem reduces to locating a single cluster boundary. Using a binary search over division intervals:
        \begin{itemize}
            \item The midpoint of centroids behaves monotonically, allowing us to refine the search scope iteratively.
            \item The algorithm achieves a total complexity of \(O(\log n)\) and guarantees convergence to a local minimum of Lloyd's algorithm.
        \end{itemize}
\end{itemize}

Both algorithms demonstrate how the structure of one-dimensional data enables faster computations compared to traditional clustering methods. In cases with multiple local minima, the solutions produced may not be globally optimal---a limitation shared with Lloyd's algorithm. Nonetheless, the proposed methods strike an effective balance between accuracy and computational efficiency, making them highly suitable for practical applications.
